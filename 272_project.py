# -*- coding: utf-8 -*-
"""272-Project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1YvgM4ksxMIXxuWjwWFtha6HyLVl2qmBv
"""

pip install --upgrade openai

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report
import joblib
from openai import OpenAI

df = pd.read_csv('/content/HR_comma_sep.csv')  # Adjust the path if needed
# Display basic information about the dataset
print(df.info())

# Check for missing values
print(df.isnull().sum())

# Display a sample of the data
print(df.head())

def preprocess_data():
    # Load the dataset
    df = pd.read_csv('/content/HR_comma_sep.csv')  # Replace with your dataset path

    # Rename columns for clarity
    df.columns = ['satisfaction_level', 'last_evaluation', 'number_project',
                  'average_monthly_hours', 'time_spend_company', 'work_accident',
                  'left', 'promotion_last_5years', 'department', 'salary']

    # Convert categorical variables to numerical
    df['salary'] = df['salary'].map({'low': 0, 'medium': 1, 'high': 2})
    df = pd.get_dummies(df, columns=['department'])

    # Split features and target
    X = df.drop('left', axis=1)
    y = df['left']

    return X, y

# Train the model
def train_model(X, y):
    # Split the dataset
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    # Scale features
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)

    # Train Random Forest model
    rf_model = RandomForestClassifier(n_estimators=100, random_state=42)
    rf_model.fit(X_train_scaled, y_train)

    # Evaluate the model
    y_pred = rf_model.predict(X_test_scaled)
    print(f"Model Accuracy: {accuracy_score(y_test, y_pred):.2f}")
    print("\nClassification Report:")
    print(classification_report(y_test, y_pred))

    # Save the model, scaler, and feature columns
    joblib.dump(rf_model, 'rf_model.joblib')
    joblib.dump(scaler, 'scaler.joblib')
    joblib.dump(X.columns, 'X_columns.joblib')
    print("Model and scaler saved successfully!")
    return rf_model, scaler

def predict_retention(candidate_data):
    # Load the saved model, scaler, and feature columns
    rf_model = joblib.load('rf_model.joblib')
    scaler = joblib.load('scaler.joblib')
    X_columns = joblib.load('X_columns.joblib')

    # Prepare the input data
    input_df = pd.DataFrame([candidate_data])
    if 'salary' in input_df.columns:
        input_df['salary'] = input_df['salary'].map({'low': 0, 'medium': 1, 'high': 2})
    input_df = pd.get_dummies(input_df, columns=['department'])

    # Ensure all columns from training are present
    for col in X_columns:
        if col not in input_df.columns:
            input_df[col] = 0

    # Reorder columns to match training data
    input_df = input_df[X_columns]

    # Scale the input data
    input_scaled = scaler.transform(input_df)

    # Make predictions
    prediction = rf_model.predict(input_scaled)
    probability = rf_model.predict_proba(input_scaled)[0][1]

    return prediction[0], probability

client = OpenAI(api_key="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")  # Replace with your API key

def generate_recommendations(prediction, probability, employee_data):
    """
    Generate tailored recommendations for employee retention.
    """
    prompt = f"""
    Based on the following prediction:
    Prediction: {'Likely to leave' if prediction == 1 else 'Likely to stay'}.
    Probability of leaving: {probability:.2f}.
    Employee details: {employee_data}.

    Provide actionable recommendations for HR to improve retention or engagement.
    """
    try:
        response = client.chat.completions.create(
            model="gpt-4",  # Use "gpt-3.5-turbo" if preferred
            messages=[
                {"role": "system", "content": "You are an HR assistant providing retention advice."},
                {"role": "user", "content": prompt}
            ],
            max_tokens=275,
            temperature=0.7
        )
        return response.choices[0].message.content.strip()
    except Exception as e:
        return f"Error generating recommendations: {str(e)}"

def update_model(new_data, actual_outcome):
    # Load existing model, scaler, and feature columns
    rf_model = joblib.load('rf_model.joblib')
    scaler = joblib.load('scaler.joblib')
    X_columns = joblib.load('X_columns.joblib')

    # Prepare new data
    new_df = pd.DataFrame([new_data])
    if 'salary' in new_df.columns:
        new_df['salary'] = new_df['salary'].map({'low': 0, 'medium': 1, 'high': 2})
    new_df = pd.get_dummies(new_df, columns=['department'])

    # Ensure all columns from original training data are present
    for col in X_columns:
        if col not in new_df.columns:
            new_df[col] = 0

    # Reorder columns to match original training data
    new_df = new_df[X_columns]

    # Scale the new data
    new_data_scaled = scaler.transform(new_df)

    # Update the model with the new data
    rf_model.fit(new_data_scaled, [actual_outcome])

    # Save the updated model
    joblib.dump(rf_model, 'rf_model.joblib')
    print("Model updated successfully!")

def get_candidate_data():
    print("Enter candidate information:")
    return {
        'satisfaction_level': float(input("Satisfaction level (0-1): ")),
        'last_evaluation': float(input("Last evaluation score (0-1): ")),
        'number_project': int(input("Number of projects: ")),
        'average_monthly_hours': int(input("Average monthly hours: ")),
        'time_spend_company': int(input("Years in the company: ")),
        'work_accident': int(input("Work accident (0 or 1): ")),
        'promotion_last_5years': int(input("Promotion in last 5 years (0 or 1): ")),
        'department': input("Department: "),
        'salary': input("Salary (low/medium/high): ")
    }

def main():
    X, y = preprocess_data()
    train_model(X, y)

    while True:
        print("\nAI-Driven Talent Acquisition and Retention Tool")
        print("1. Predict candidate retention")
        print("2. Update model with new data")
        print("3. Exit")
        choice = input("Enter your choice (1-3): ")

        if choice == '1':
            candidate_data = get_candidate_data()
            prediction, probability = predict_retention(candidate_data)

            print(f"\nThe candidate is {'likely to stay' if prediction == 0 else 'at risk of leaving'}.")
            print(f"Probability of leaving: {probability:.2f}")

            recommendations = generate_recommendations(prediction, probability, candidate_data)
            print("\nRecommendations:")
            print(recommendations)

        elif choice == '2':
            candidate_data = get_candidate_data()
            actual_outcome = int(input("Did the employee leave? (0 for No, 1 for Yes): "))
            update_model(candidate_data, actual_outcome)

        elif choice == '3':
            print("Thank you for using the AI-Driven Talent Acquisition and Retention Tool.")
            break
        else:
            print("Invalid choice. Please try again.")

if __name__ == "__main__":
    main()

!pip install joblib pandas scikit-learn



df.columns = ['satisfaction_level', 'last_evaluation', 'number_project',
              'average_monthly_hours', 'time_spend_company', 'work_accident',
              'left', 'promotion_last_5years', 'department', 'salary']

# Convert categorical variables to numerical
df['salary'] = df['salary'].map({'low': 0, 'medium': 1, 'high': 2})
df = pd.get_dummies(df, columns=['department'])

# Split features and target
X = df.drop('left', axis=1)
y = df['left']

# Split into train and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Scale the features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Train a Random Forest model
rf_model = RandomForestClassifier(n_estimators=100, random_state=42)
rf_model.fit(X_train_scaled, y_train)

print("Model trained successfully!")

import joblib

# Save the RandomForestClassifier model
joblib.dump(rf_model, 'rf_model.joblib')
print("Model saved as rf_model.joblib")

# Save the StandardScaler
joblib.dump(scaler, 'scaler.joblib')
print("Scaler saved as scaler.joblib")

# Get feature importance for RandomForest
importances = rf_model.feature_importances_
indices = np.argsort(importances)[::-1]

# Print the top 5 most important features
print("Top 5 most important features:")
for i in range(5):
    print(f"{X.columns[indices[i]]}: {importances[indices[i]]}")

import joblib

# Load the saved model and scaler
loaded_model = joblib.load('rf_model.joblib')
loaded_scaler = joblib.load('scaler.joblib')

# Test the loaded model and scaler
sample_data = X_test.iloc[0:1]  # Use a sample row from the test set
scaled_data = loaded_scaler.transform(sample_data)
prediction = loaded_model.predict(scaled_data)
print(f"Prediction: {prediction}")

from google.colab import files

# Download the model and scaler files
files.download('rf_model.joblib')
files.download('scaler.joblib')

